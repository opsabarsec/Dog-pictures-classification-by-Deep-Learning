{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet V2 - Stanford Dogs dataset classification with transfer learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c4003dd-30c3-4fb4-ad7e-ff226a11ada7",
    "_uuid": "74bb5ae0-8554-4db7-9115-0c0721a955a3"
   },
   "source": [
    "#### Classifying 120 classes without pre-trained models from *keras.applications* can be a challenging task. In this notebook, I am trying to accomplish this by using MobileNet architectures to create a deep enough CNN but as fast as possible CNN for this task. Feedback is welcome!\n",
    "#### What I did with this model:\n",
    "\n",
    "* Transfer learning based on MobileNet CNN but the size is reduced substantially to save training time\n",
    "* Using NAdam optimization (Adam with Nesterov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b05acba4-7107-488b-9b5f-e881c34ddde1",
    "_uuid": "567924ea-408a-433c-9bb1-28c3fe74d2b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import math\n",
    "import cv2\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications \n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras import backend as k \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.initializers import *\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "8798d6fa-a76a-4220-a633-49c7a4d85d58",
    "_kg_hide-input": false,
    "_uuid": "17c5d401-7c40-4734-9533-a0cb7aba1ec5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "image_path = '/media/marco/DATA/OC_Machine_learning/section_6/DATA/Images/'\n",
    "num_of_categories = 120\n",
    "image_size = 224\n",
    "batch_size = 64\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(20580*0.8)\n",
    "test_size = int(20580*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "2af8c132-b561-4a89-831b-12e817142dc6",
    "_uuid": "781b32ed-7a91-4a75-b9fd-2153ec04fe16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Image loading method 1: image_datagen and flow_from_directory\n",
    "\n",
    "\n",
    "\n",
    "def image_datagen_load():\n",
    "\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=30, width_shift_range=0.2,\n",
    "        height_shift_range=0.2, brightness_range=[0.9,1.1], zoom_range=0.1, fill_mode='constant', cval=0.0, horizontal_flip=True, \n",
    "        rescale=1/255., preprocessing_function = preprocessImage, validation_split=0.2, dtype='float32')\n",
    "\n",
    "    train_datagen = datagen.flow_from_directory(image_path,\n",
    "        target_size=(image_size, image_size), color_mode='rgb', classes=None, interpolation='hamming',\n",
    "        class_mode='categorical', batch_size=batch_size, shuffle=True, subset='training')\n",
    "\n",
    "    test_datagen = datagen.flow_from_directory(image_path,\n",
    "        target_size=(image_size, image_size), color_mode='rgb', classes=None, interpolation='hamming',\n",
    "        class_mode='categorical', batch_size=batch_size, shuffle=False, subset='validation')\n",
    "    \n",
    "    return train_datagen, test_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16508 images belonging to 120 classes.\n",
      "Found 4072 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "# image loading \n",
    "\n",
    "train, test = image_datagen_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "70afbf92-2ee0-4f93-8846-a89ae0a2b2dc",
    "_uuid": "364a1f19-626f-4d5f-88a3-570019db9de7"
   },
   "outputs": [],
   "source": [
    "filters = 32\n",
    "kernel_size = (3,3)\n",
    "stride = (1,1)\n",
    "pool_size = (3,3)\n",
    "l2 = tf.keras.regularizers.l2(0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1280)              5120      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1280)              1639680   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               153720    \n",
      "=================================================================\n",
      "Total params: 4,056,504\n",
      "Trainable params: 4,019,832\n",
      "Non-trainable params: 36,672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = applications.mobilenet_v2.MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3), )\n",
    "base_model.trainable = True\n",
    "\n",
    "seed = 42\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(num_of_categories, activation='softmax',  kernel_initializer=glorot_uniform(seed), bias_initializer='zeros'))\n",
    "model.add(Dense(1280, activation='relu',  kernel_initializer=glorot_uniform(seed), bias_initializer='zeros'))\n",
    "\n",
    "\n",
    "model.add(Dense(num_of_categories, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "\n",
    "#model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "optimizer = Adam(lr=0.0001)\n",
    "#SGD(lr=1.62e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, 0.00014715177646857695, 0.00030122571620038027, 0.00043307290635716066, 0.0005130312413993676, 0.0005377003383729307, 0.0005178851256927623, 0.00046888035555159497, 0.00040492292381893205, 0.00033689734995427336, 0.00027197463922978366, 0.00021416418806397342, 0.00016515279534858634, 0.00012511020567407963, 9.333298746244693e-05, 6.870274619443443e-05, 4.998200484746474e-05, 3.5986298871675764e-05, 2.567043505998692e-05, 1.815997190499394e-05, 1.815997190499394e-05, 1.815997190499394e-05, 1.815997190499394e-05, 1.815997190499394e-05, 1.815997190499394e-05, 1.815997190499394e-05, 1.815997190499394e-05, 1.815997190499394e-05, 1.815997190499394e-05, 1.815997190499394e-05]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################\n",
    "total_epoch = 50\n",
    "learning_rate_init = 0.0001\n",
    "###################\n",
    "\n",
    "def scheduler_2(epoch):\n",
    "    epoch += 1\n",
    "   \n",
    "    if epoch == 1:\n",
    "        return learning_rate_init\n",
    "    \n",
    "    elif epoch >= 2 and epoch <= 20:\n",
    "        return (0.5*epoch**3)*math.exp(-0.5*epoch)*learning_rate_init\n",
    "    \n",
    "    else:\n",
    "        return scheduler_2(20-1)\n",
    "    \n",
    "\n",
    "stage = [i for i in range(0,30)]\n",
    "learning_rate = [scheduler_2(x) for x in stage]\n",
    "plt.plot(stage, learning_rate)\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.Model(inputs=base_model.input, outputs = predictions, name='model')\n",
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "metrics = [keras.metrics.CategoricalAccuracy(name='acc')]\n",
    "model.compile(loss=loss,optimizer=optimizer,metrics=metrics)\n",
    "\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "6e65fd6a-2266-4192-9e57-2ec932305fba",
    "_uuid": "f1a352f0-3a1e-47ce-ab54-9d02ba37f652"
   },
   "outputs": [],
   "source": [
    "scheduler = keras.callbacks.LearningRateScheduler(scheduler_2, verbose=1)\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_acc',mode='max',verbose=1,patience=5,restore_best_weights=True)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('temp',save_weights_only=True,monitor='val_acc',mode='max',save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 1164s 5s/step - loss: 2.4431 - acc: 0.4256 - val_loss: 5.0202 - val_acc: 0.0083 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00014715177646857695.\n",
      "Epoch 2/50\n",
      " 63/257 [======>.......................] - ETA: 13:34 - loss: 1.1333 - acc: 0.6671"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(train, batch_size=batch_size, epochs=total_epoch, callbacks=[scheduler,earlystop,checkpoint],\n",
    "                    validation_data=test, steps_per_epoch=train_size//batch_size, validation_steps=test_size//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save('dog_breed_mobilenetV2.h5', overwrite=True)\n",
    "model.save_weights('dog_breed_mobilenetV2_weights.h5', overwrite=True)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-925e4bc0ca28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max validation accuracy during training: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'min validation loss during training: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "print('max validation accuracy during training: ',max(history.history['val_acc']))\n",
    "print('min validation loss during training: ',min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test, batch_size=batch_size, steps=test_size//batch_size, verbose=1)\n",
    "best_predictions = np.argmax(pred, axis=-1)\n",
    "print(\"best_predictions \", best_predictions )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_id={}\n",
    "id_to_class={}\n",
    "for key, value in test.class_indices.items():\n",
    "    class_to_id[key[10:]] = value\n",
    "    id_to_class[value] = key[10:]\n",
    "\n",
    "report = classification_report(test.classes, best_predictions, target_names=class_to_id)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15a4554c-5ac0-4b47-bcbb-3e996b05ab8e",
    "_uuid": "2c213fee-9d59-4087-b547-d8ac9f658f05",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('model accuracy')\n",
    "plt.plot(history.history['acc'],label='train accuracy')\n",
    "plt.plot(history.history['val_acc'],label='test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fd404dfb-928c-4b65-bab1-404d855bb296",
    "_uuid": "9e96a88d-af80-4268-ba95-adf8263defe9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('model loss')\n",
    "plt.plot(history.history['loss'],label='train loss')\n",
    "plt.plot(history.history['val_loss'],label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breed_list = os.listdir(image_path)\n",
    "print(\"num. breeds total:\", len(breed_list))\n",
    "filtered_breeds = [breed.split('-',1)[1] for breed in breed_list] \n",
    "\n",
    "roundpred = np.around(pred, decimals=1)\n",
    "df_pred = pd.DataFrame(roundpred, columns = filtered_breeds[:num_of_categories])\n",
    "df_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breed_pred = df_pred.apply(lambda s, n: pd.Series(s.nlargest(n).index), axis=1, n=3)\n",
    "df_breed_pred.columns = ['1st_prob_breed','2nd_prob_breed','3rd_prob_breed']\n",
    "df_breed_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = df_pred.apply(np.sort, axis=1).apply(lambda df_pred: df_pred[-3:]).apply(pd.Series)\n",
    "prob_df.columns = ['3rd_prob','2nd_prob','1st_prob']\n",
    "prob_df = prob_df*100\n",
    "prob_df = prob_df.astype(int)\n",
    "prob_df = pd.concat([prob_df, df_breed_pred], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prob_df['final']= prob_df[\"1st_prob_breed\"].astype(str) +\" \"+ prob_df[\"1st_prob\"].astype(str)+\"%, \"+prob_df[\"2nd_prob_breed\"].astype(str) +\" \"+ prob_df[\"2nd_prob\"].astype(str)+\"%, \"+prob_df[\"3rd_prob_breed\"].astype(str) +\" \"+ prob_df[\"3rd_prob\"].astype(str)+\"%\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true_n = test.classes\n",
    "#df_test = pd.DataFrame(y_true, columns = filtered_breeds[:num_of_categories])\n",
    "#given_df = df_test.apply(lambda s, n: pd.Series(s.nlargest(n).index), axis=1, n=1)\n",
    "#given_df.head()\n",
    "y_true = [id_to_class[i] for i in y_true_n]\n",
    "#print (','.join(y_true))\n",
    "df_given = pd.DataFrame(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "r = np.random.randint(low=1, high=100, size=9)\n",
    "for i in r:\n",
    "    n += 1 \n",
    "    \n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    name = test.filepaths[i]\n",
    "    plt.imshow(mpimg.imread(name))\n",
    "    plt.title(y_true[i])\n",
    " \n",
    "    plt.xlabel(prob_df.iloc[i,6], wrap=True, color = \"r\")\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "imageno=np.random.random_integers(low=0, high=test.samples)\n",
    "\n",
    "name = test.filepaths[imageno]\n",
    "print(name)\n",
    "plt.imshow(mpimg.imread(name))\n",
    "\n",
    "img = Image.open(test.filepaths[imageno]).resize((224, 224))\n",
    "probabilities = model.predict(preprocess_input(np.expand_dims(img, axis=0)))\n",
    "breed_list = tuple(zip(test.class_indices.values(), test.class_indices.keys()))\n",
    "\n",
    "for i in probabilities[0].argsort()[-5:][::-1]: \n",
    "    print(probabilities[0][i], \"  :  \" , breed_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

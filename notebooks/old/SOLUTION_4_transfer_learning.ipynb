{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mSf2FogINuWV"
   },
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exâ€šcutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "# After uploading the zip data file, unzip it\n",
    "# Uploading can take a while\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:26.377707Z",
     "start_time": "2019-06-16T15:18:25.264617Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "_ZHdX_gCEJsT",
    "outputId": "e1f84307-3cd6-44a4-ae32-ea725d076198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\benja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\benja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\benja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\benja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\benja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.1.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.7.0\n",
      "WARNING:tensorflow:From <ipython-input-2-5a168099d0c2>:26: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MaBQIKA7gbOR"
   },
   "source": [
    "The objective in this notebook is to manipulate pre-trained networks and have a clear understanding of the benefits of transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kr5D1gXf5hwV"
   },
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFfwQ1S1NuWd"
   },
   "source": [
    "The problem we are trying to solve here is to classify RGB  images into their 2 categories (\"Puma\" or \"Nike\") by taking advantage of transfer learning. \n",
    "\n",
    "For this exercice you will work with MobileNetV2.\n",
    "\n",
    "First look at the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:29.142452Z",
     "start_time": "2019-06-16T15:18:29.136627Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "qtv-APbJNubo",
    "outputId": "fc2a0f2f-2d0d-42dc-9ff3-e087dff7358d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Nike\n",
      "data\\Puma\n"
     ]
    }
   ],
   "source": [
    "data_root = pathlib.Path(\"./data/\")\n",
    "for item in data_root.iterdir():\n",
    "      print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yt95MeFdgbOc"
   },
   "source": [
    "Let's list all the images' path : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:31.272136Z",
     "start_time": "2019-06-16T15:18:31.259210Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_nqXDOpvgbOj",
    "outputId": "e775d0a8-3b01-4782-e940-fbece2656690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# images :  1999 \n",
      "\n",
      "Extract of image paths : \n",
      " ['data\\\\Puma\\\\001026.jpg', 'data\\\\Nike\\\\000825.jpg', 'data\\\\Puma\\\\000752.jpg', 'data\\\\Puma\\\\000071.jpg']\n"
     ]
    }
   ],
   "source": [
    "all_image_paths = list(data_root.glob('*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "random.shuffle(all_image_paths)\n",
    "\n",
    "DATASET_SIZE = len(all_image_paths)\n",
    "print(\"# images : \", DATASET_SIZE,\"\\n\")\n",
    "print(\"Extract of image paths : \\n\", all_image_paths[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXw9Umoi5-Ga"
   },
   "source": [
    "## Determine label for each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLz0O7-ngbOr"
   },
   "source": [
    "From the structure of the folder we are able to identify the different labels : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:33.834200Z",
     "start_time": "2019-06-16T15:18:33.822556Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5jo06qaX49GG",
    "outputId": "dc5d8793-baad-444c-8663-f33873d6d4b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nike', 'Puma']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all the labels\n",
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juE6G2o2gbOz"
   },
   "source": [
    "Let's map these labels to indexes and keep track of it in dictionnaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:35.797679Z",
     "start_time": "2019-06-16T15:18:35.793728Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OrTTjCLR6Joi",
    "outputId": "1250cdd3-7deb-41ad-8819-747ffb45982c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to Index :  {'Nike': 0, 'Puma': 1} \n",
      "Index to Label :  {0: 'Nike', 1: 'Puma'}\n"
     ]
    }
   ],
   "source": [
    "# Assign an index to each label in a dictionnary: \n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "print(\"Label to Index : \", label_to_index, \"\\nIndex to Label : \", index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0arOYwMrgbO6"
   },
   "source": [
    "Now let's define the correct index to each image path :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:37.430752Z",
     "start_time": "2019-06-16T15:18:37.415182Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ti999-Vs6V0T",
    "outputId": "85bb62e9-3903-44e9-b66a-d0fc748a1128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 labels indices:  [1, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Associate an image to an index label (as an integer)\n",
    "all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                    for path in all_image_paths]\n",
    "\n",
    "print(\"First 10 labels indices: \", all_image_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uaJ9ladM5LOm"
   },
   "source": [
    "## Inspect the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPZwzfJ3gbPA"
   },
   "source": [
    "Let's have a look on a few images : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:39.378820Z",
     "start_time": "2019-06-16T15:18:39.364358Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cDvffueNvDyK",
    "outputId": "15b93265-4226-45d8-9168-e7e75a7ee469"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :  0  - Label :  Nike\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :  1  - Label :  Puma\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :  1  - Label :  Puma\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "    index = random.randint(0, DATASET_SIZE)\n",
    "    image_path, image_label = all_image_paths[index], all_image_labels[index]\n",
    "    display.display(display.Image(image_path))\n",
    "    print(\"index : \", image_label, \" - Label : \", index_to_label[image_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqeO6xZO9Rrf"
   },
   "source": [
    "## Build a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf1yrf8BgbPI"
   },
   "source": [
    "Now let's build a tf.data.Dataset from the list of images and labels associated.\n",
    "\n",
    "In this exerice you will use 80% of your data in the training and save the last 20% of them for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:41.867184Z",
     "start_time": "2019-06-16T15:18:41.864080Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "FSP9gsZ09RZi",
    "outputId": "2e7b903b-e72e-467f-a73b-27f8f28ab18a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# images :  1999 \n",
      "\n",
      "Train size :  1599 \n",
      "Test size :  400\n"
     ]
    }
   ],
   "source": [
    "p = 0.8\n",
    "train_size = int(p * DATASET_SIZE)\n",
    "test_size = DATASET_SIZE - train_size\n",
    "\n",
    "print(\"# images : \", DATASET_SIZE,\"\\n\")\n",
    "print(\"Train size : \", train_size, \"\\nTest size : \",test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuM2d_4Li8Ad"
   },
   "source": [
    "We have implemented the input preprocessing function, composed of the following steps :\n",
    "- read the file as an image\n",
    "- decode the jpeg file, setting the number of channels at 3 as it's a RGB image\n",
    "- resize the image in (224,224) as required in MobileNetV2 specifications\n",
    "- cast image to float32\n",
    "- divide image values by 255\n",
    "- one_hot encoding of the label\n",
    "\n",
    "Documentation : \n",
    "- https://www.tensorflow.org/api_docs/python/tf/io/read_file\n",
    "- https://www.tensorflow.org/versions/r1.11/api_docs/python/tf/image/decode_jpeg\n",
    "- https://www.tensorflow.org/api_docs/python/tf/dtypes/cast\n",
    "- https://www.tensorflow.org/api_docs/python/tf/math/divide\n",
    "- https://www.tensorflow.org/api_docs/python/tf/one_hot/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:44.644887Z",
     "start_time": "2019-06-16T15:18:44.638529Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "awgM7GD_Nub1"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "NUM_CAT = 2\n",
    "\n",
    "def _preprocess_inputs(img_path, label):\n",
    "    # Convert path to image\n",
    "    image = tf.io.read_file(img_path)\n",
    "    \n",
    "    # Resize images\n",
    "    img_resized = tf.image.decode_jpeg(image, channels=3)\n",
    "    img_resized = tf.image.resize(img_resized, (IMG_SIZE,IMG_SIZE))\n",
    "\n",
    "    # Cast image to float32\n",
    "    img_float = tf.cast(img_resized, tf.float32)\n",
    "    \n",
    "    # Divide image values by 255\n",
    "    img_norm = tf.divide(img_float, 255)\n",
    "   \n",
    "    label_one_hot = tf.one_hot(label,depth=NUM_CAT)\n",
    "    return img_norm, label_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T13:15:21.434836Z",
     "start_time": "2019-06-13T13:15:21.425834Z"
    },
    "colab_type": "text",
    "id": "ITnDtNi1gbPX"
   },
   "source": [
    "We now use tf.data to create the input data pipeline. It will be composed of the following steps :\n",
    "\n",
    "- Create the Dataset. Since we already have in-memory data, we will use the from_tensor_slices function\n",
    "- map the dataset with the preprocessing function implemented above\n",
    "- map the result with the image augmentation function\n",
    "- Add the shuffle, repeat, batch and prefect steps to configure training\n",
    "\n",
    "Implement the input data pipeline with the above descriptions for the training set\n",
    "\n",
    "Documentation :\n",
    "\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:48.097108Z",
     "start_time": "2019-06-16T15:18:48.071659Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jt9wvAcsNucK",
    "outputId": "467540d5-2975-47f4-d2a0-8a29ee382c45"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = DATASET_SIZE\n",
    "\n",
    "# Build a tf.data.Dataset\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((all_image_paths[:train_size], all_image_labels[:train_size]))\n",
    "# Map for input preprocessing\n",
    "ds_train = ds_train.map(_preprocess_inputs)\n",
    "# Shuffle / Repeat / Batch / Prefetch\n",
    "ds_train = ds_train.shuffle(SHUFFLE_BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnzuO_X4kp4o"
   },
   "source": [
    "The input data pipeline for the test set is quite similar, except there is no need to use the repeat and prefetch steps. Moreover, the image augmentation phase is not necessary for the test set, as we want the real images to be used to evaluate the results.\n",
    "\n",
    "Here the input data pipeline for the test set : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:54.237432Z",
     "start_time": "2019-06-16T15:18:54.222835Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8TfY-sA4NucM"
   },
   "outputs": [],
   "source": [
    "# Build a dataset\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((all_image_paths[train_size:], all_image_labels[train_size:]))\n",
    "# Map\n",
    "ds_test = ds_test.map(_preprocess_inputs)\n",
    "# Shuffle / Batch\n",
    "ds_test = ds_test.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvx3Xk3eAN-w"
   },
   "source": [
    "# Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MObXOE4KRLZ6"
   },
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOadhqr4gbPq",
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "Using what we have learned so far, build a model that can distinguish a Nike shoe from a Puma one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:18:57.185342Z",
     "start_time": "2019-06-16T15:18:56.694599Z"
    },
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "#Example of a basic CNN model\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(dropout_rate/2.))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(dropout_rate/2.))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(NUM_CAT, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T13:45:45.644642Z",
     "start_time": "2019-06-13T13:45:45.639641Z"
    },
    "colab_type": "text",
    "id": "HYLIVDeTgbPz"
   },
   "source": [
    "You can have a look at your model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:16:13.642846Z",
     "start_time": "2019-06-16T15:16:13.639337Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_0-HEUpggbP1",
    "outputId": "07d1a91d-a139-4744-cfd2-51a0f31beff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 224, 224, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 37,954\n",
      "Trainable params: 37,250\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nsvHD-EZgbP5",
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:19:01.539109Z",
     "start_time": "2019-06-16T15:19:01.426686Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Wf1hTuGygbP5",
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              optimizer=SGD(lr=1e-2, momentum=0.9, decay=1e-6),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=[\"acc\"]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T13:54:28.984488Z",
     "start_time": "2019-06-13T13:54:28.980489Z"
    },
    "colab_type": "text",
    "id": "4kWUajujgbP8"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:20:12.376560Z",
     "start_time": "2019-06-16T15:19:06.075135Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "T_s0UV_9gbP8",
    "outputId": "fc83c8da-7a28-4c45-d3b0-e2a9e7262bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 49 steps, validate for 12 steps\n",
      "Epoch 1/10\n",
      " 1/49 [..............................] - ETA: 5:10"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-16-7c9cb4476036>:7) ]] [Op:__inference_distributed_function_1754]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7c9cb4476036>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                    \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_size\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                    \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                    \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                   )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-16-7c9cb4476036>:7) ]] [Op:__inference_distributed_function_1754]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "history= model.fit(ds_train,\n",
    "                   epochs=NUM_EPOCHS,\n",
    "                   steps_per_epoch=int(train_size / BATCH_SIZE),\n",
    "                   validation_data=ds_test,\n",
    "                   validation_steps=int(test_size / BATCH_SIZE),\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mx896WBcRlGO"
   },
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdV1J30bgbQl"
   },
   "source": [
    "Now you can analyse the performance of your algorithm : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:20:19.538905Z",
     "start_time": "2019-06-16T15:20:19.297292Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "rvo1_ybNNucu",
    "outputId": "d331178f-01eb-4394-df51-1edc44e9eb28"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1.01])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "#plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WosQdYg5gbP_"
   },
   "source": [
    "# Start real transfer learning : load a pre-trained model AND download the weights\n",
    "## Load the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_V5EYiccgbQA",
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "This time you will use the pre-trained algorithm and fine tune it.\n",
    "It means you will load the weights of the algorithm learned when it was trained on ImageNet images and add layers on top of the model.\n",
    "\n",
    "Change the parameters with the following ones : \n",
    "- input shape : (224,224,3)\n",
    "- include top : False\n",
    "- pooling : \"max\"\n",
    "- weights = \"imagenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:20:35.441701Z",
     "start_time": "2019-06-16T15:20:33.439810Z"
    },
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Create the base model from the model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
    "                                               include_top=False,\n",
    "                                               pooling=\"avg\",\n",
    "                                               weights='imagenet',\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAX8lolnw6MU"
   },
   "source": [
    "In order to keep the same weight you need to freeze the model by setting the parameter base_model.trainable to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:20:35.592542Z",
     "start_time": "2019-06-16T15:20:35.587871Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EeB8XcCxNucd"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQSf2nhvgbQN"
   },
   "source": [
    "Check the structure of the model, you will see that the last layers have changed. Indeed you have dropped the top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:20:37.495907Z",
     "start_time": "2019-06-16T15:20:37.419823Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "x3mTJMd5RCd-",
    "outputId": "a5ca12e9-e9a7-43b6-b39e-cef934c74eff"
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YNf-JOZmRbBd"
   },
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEA8VH2RgbQQ",
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "You are free to add layers in your model to fine tune it : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:21:08.751175Z",
     "start_time": "2019-06-16T15:21:07.273952Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "H9luWKIdxEq9",
    "outputId": "441d60fb-5195-4746-d012-25742f124b0c",
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(NUM_CAT, \"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TotHK_wtReCl"
   },
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPr-oXqIPN4Y",
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "As you are fine-tuning the model we advise you to choose small learning rate in order to converge.\n",
    "\n",
    "Here again, you are free to use the optimizer you want :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:21:26.524620Z",
     "start_time": "2019-06-16T15:21:26.448906Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XpzKBKSANucg",
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              optimizer=SGD(lr=1e-3, momentum=0.9, decay=1e-6),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=[\"acc\"]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-NCwx12RiaH"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kzwgtTGgbQg"
   },
   "source": [
    "Since we're using a pretrained model, we have a lot less parameters to train and we can use a bigger batch size so let's push it ot 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:21:30.056087Z",
     "start_time": "2019-06-16T15:21:30.042795Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jt9wvAcsNucK",
    "outputId": "467540d5-2975-47f4-d2a0-8a29ee382c45"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = DATASET_SIZE\n",
    "\n",
    "# Build a tf.data.Dataset\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((all_image_paths[:train_size], all_image_labels[:train_size]))\n",
    "# Map for input preprocessing\n",
    "ds_train = ds_train.map(_preprocess_inputs)\n",
    "# Shuffle / Repeat / Batch / Prefetch\n",
    "ds_train = ds_train.shuffle(SHUFFLE_BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:21:30.489649Z",
     "start_time": "2019-06-16T15:21:30.469351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a dataset\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((all_image_paths[train_size:], all_image_labels[train_size:]))\n",
    "# Map\n",
    "ds_test = ds_test.map(_preprocess_inputs)\n",
    "# Shuffle / Batch\n",
    "ds_test = ds_test.shuffle(SHUFFLE_BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train you model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:22:26.093627Z",
     "start_time": "2019-06-16T15:21:32.833867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "1k7NNVE7Nucj",
    "outputId": "6dc2009c-f78d-44f3-bc30-476dac05a7c0"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "history= model.fit(ds_train,\n",
    "                   epochs=NUM_EPOCHS,\n",
    "                   steps_per_epoch = int(train_size / BATCH_SIZE)+1,\n",
    "                   shuffle=True,\n",
    "                   validation_data=ds_test,\n",
    "                   validation_steps=int(test_size / BATCH_SIZE)+1\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mx896WBcRlGO"
   },
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdV1J30bgbQl"
   },
   "source": [
    "Now you can analyse the performance of your algorithm : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:22:30.609766Z",
     "start_time": "2019-06-16T15:22:30.371079Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "rvo1_ybNNucu",
    "outputId": "d331178f-01eb-4394-df51-1edc44e9eb28"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1.01])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "#plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8C74teQ6yJz"
   },
   "source": [
    "# Another method : import pre-trained models from tf.hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJ-t8iyTgbQq"
   },
   "source": [
    "Another way to load pre-trained models is to use tf.hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLs_ntfkSAXn"
   },
   "source": [
    "## Load the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZKt29WrgbQs"
   },
   "source": [
    "You only need to select the link of the model you want to exploit like below : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T14:26:10.195120Z",
     "start_time": "2019-06-13T14:26:10.191122Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "O6LAWk9122Wn"
   },
   "outputs": [],
   "source": [
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/2\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ch-YflxgbQu",
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "Turn this module into a layer with the following parameters : \n",
    "- the path to module\n",
    "- trainable = False\n",
    "- input_shape = (224, 224, 3)\n",
    "\n",
    "Documentation :\n",
    "https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(x):\n",
    "    feature_extractor_module = hub.Module(feature_extractor_url)\n",
    "    return feature_extractor_module(x)\n",
    "\n",
    "IMAGE_SIZE = hub.get_expected_image_size(hub.Module(feature_extractor_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZWaBFdtPr0y",
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "features_extractor_layer = tf.keras.layers.Lambda(feature_extractor, input_shape=IMAGE_SIZE+[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goxwKExBgbQx"
   },
   "outputs": [],
   "source": [
    "features_extractor_layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cW5smWikSGS7"
   },
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfjDiHSrgbQz",
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "You can add the layers you want at the top of your model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "ungvmcPZ28s7",
    "outputId": "3b50b650-e831-4241-fa9f-3f741589f584",
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(features_extractor_layer)\n",
    "\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.add(Dense(NUM_CAT, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r43QCVuqSITO"
   },
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zT-VTbE0gbQ6",
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "As usual, you compile the model with the optimizer you want.\n",
    "Keep in mind that if you fine-tune the model it is recommended to use small learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E394wmx94m6N",
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=1e-3, momentum=0.9, decay=1e-6)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfkAHZpiSKZc"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdKlMxrFgbQ_"
   },
   "source": [
    "It's time to train your model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "id": "Luk3OnrwicPO",
    "outputId": "baaf1e45-c1c8-48b5-fa15-8a4a8691d615"
   },
   "outputs": [],
   "source": [
    "history = model.fit(ds_train,\n",
    "                   epochs=10,\n",
    "                   steps_per_epoch=int(train_size/BATCH_SIZE),\n",
    "                   validation_data=ds_test,\n",
    "                   validation_steps=int(test_size/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5rS1oFQSMbl"
   },
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T14:34:13.988425Z",
     "start_time": "2019-06-13T14:34:13.982441Z"
    },
    "colab_type": "text",
    "id": "4peqdYkygbRD"
   },
   "source": [
    "Finally you can analyse the performance of your model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "Ev9BfTK0MXsZ",
    "outputId": "0bb0c70b-597b-4932-baec-a61ec7be78f2"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "#plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Formation_DL_Image_Transfer_Learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

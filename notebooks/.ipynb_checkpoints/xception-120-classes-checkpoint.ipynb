{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I will be performing following steps :\n",
    "1. Importing the data \n",
    "2. Data Argumentation And Visualization \n",
    "3. Importing the Xception ( Transfer learning ) \n",
    "4. Fully Connected layer \n",
    "5. Model Training \n",
    "6. Accuracy And Loss Visualization \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ABOUT THE DATASET **\n",
    "\n",
    "The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization. It was originally collected for fine-grain image categorization, a challenging problem as certain dog breeds have near identical features or differ in colour and age.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. IMPORTING THE LIBRARIES AND DATA **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vK21CPNVcopb",
    "outputId": "f93d7bdc-5cf0-4b8b-8f50-dc041312f62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all libraries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from tf_explain.core.activations import ExtractActivations\n",
    "\n",
    "from tensorflow.keras.applications.xception import decode_predictions\n",
    "%matplotlib inline\n",
    "print(\"Loaded all libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h2ckocrJdM9K",
    "outputId": "24145e74-7114-4a08-b200-28c8135215e9"
   },
   "outputs": [],
   "source": [
    "image_path = '/media/marco/DATA/OC_Machine_learning/section_6/DATA/Images/'\n",
    "num_of_categories = 120\n",
    "image_size = 299\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h2ckocrJdM9K",
    "outputId": "24145e74-7114-4a08-b200-28c8135215e9"
   },
   "outputs": [],
   "source": [
    "def preprocessImage(img):\n",
    "    img_RGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
    "    return preprocess_input(img)\n",
    "\n",
    "def image_datagen_load():\n",
    "\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=30, width_shift_range=0.2,\n",
    "        height_shift_range=0.2, brightness_range=[0.9,1.1], zoom_range=0.1, fill_mode='constant', cval=0.0, horizontal_flip=True, \n",
    "        rescale=1/255., preprocessing_function = preprocessImage, validation_split=0.2, dtype='float32')\n",
    "\n",
    "    train_datagen = datagen.flow_from_directory(image_path,\n",
    "        target_size=(image_size, image_size), color_mode='rgb', classes=None, interpolation='hamming',\n",
    "        class_mode='categorical', batch_size=batch_size, shuffle=True, subset='training')\n",
    "\n",
    "    test_datagen = datagen.flow_from_directory(image_path,\n",
    "        target_size=(image_size, image_size), color_mode='rgb', classes=None, interpolation='hamming',\n",
    "        class_mode='categorical', batch_size=batch_size, shuffle=False, subset='validation')\n",
    "    \n",
    "    return train_datagen, test_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h2ckocrJdM9K",
    "outputId": "24145e74-7114-4a08-b200-28c8135215e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16508 images belonging to 120 classes.\n",
      "Found 4072 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "# image loading \n",
    "\n",
    "train_ds, val_ds = image_datagen_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. MODEL PREPARATION **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 IMPORTING THE Xception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LjT2fxkzhssZ",
    "outputId": "4e854709-6a02-44a7-931a-a2b3da197a44"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#pre_trained_model = InceptionV3(input_shape=(200,200,3),include_top=False, weights='imagenet')\n",
    "pre_trained_model = tf.keras.applications.xception.Xception(weights='imagenet',include_top=True, pooling='avg')#Summary of Xception Model\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "  layer.trainable = True\n",
    "\n",
    "#pre_trained_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 FULLY CONNECTED LAYER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2EIUR-yAhslh",
    "outputId": "f3c3ddd6-980d-42d3-c6e3-f81811d75225"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "flat_dim = 5 * 5 * 2048\n",
    "\n",
    "my_model = Sequential(pre_trained_model)\n",
    "\n",
    "#my_model.add(Flatten())\n",
    "#my_model.add(Dropout(0.1)) # dropout added\n",
    "my_model.add(Dense(1032, activation='relu',input_dim=flat_dim))\n",
    "my_model.add(Dropout(0.2))\n",
    "my_model.add(Dense(512, activation='relu'))\n",
    "my_model.add(Dropout(0.2))\n",
    "my_model.add(Dense(256, activation='relu'))\n",
    "my_model.add(Dropout(0.2))\n",
    "my_model.add(Dense(120, activation='softmax'))\n",
    "\n",
    "my_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=2, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TRAINING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GlkKZkoYhsTp",
    "outputId": "6fd840fd-8ebd-43c0-c0e3-555f0b3ce716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "100/100 [==============================] - 1509s 15s/step - loss: 4.7582 - accuracy: 0.0216 - val_loss: 4.8267 - val_accuracy: 0.0156 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "100/100 [==============================] - 1505s 15s/step - loss: 4.7623 - accuracy: 0.0153 - val_loss: 4.8603 - val_accuracy: 0.0156 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 1505s 15s/step - loss: 4.7330 - accuracy: 0.0125 - val_loss: 4.8846 - val_accuracy: 0.0116 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "100/100 [==============================] - 1504s 15s/step - loss: 4.6935 - accuracy: 0.0144 - val_loss: 4.8976 - val_accuracy: 0.0144 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "100/100 [==============================] - 1505s 15s/step - loss: 4.6329 - accuracy: 0.0162 - val_loss: 4.8008 - val_accuracy: 0.0031 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "100/100 [==============================] - 1509s 15s/step - loss: 4.5830 - accuracy: 0.0178 - val_loss: 4.6478 - val_accuracy: 0.0147 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.5218 - accuracy: 0.0162 \n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 1512s 15s/step - loss: 4.5218 - accuracy: 0.0162 - val_loss: 4.5979 - val_accuracy: 0.0128 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "100/100 [==============================] - 1509s 15s/step - loss: 4.4465 - accuracy: 0.0209 - val_loss: 4.5047 - val_accuracy: 0.0231 - lr: 2.0000e-04\n",
      "Epoch 9/25\n",
      "100/100 [==============================] - 1505s 15s/step - loss: 4.3899 - accuracy: 0.0262 - val_loss: 4.4576 - val_accuracy: 0.0250 - lr: 2.0000e-04\n",
      "Epoch 10/25\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.3500 - accuracy: 0.0344 \n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 1508s 15s/step - loss: 4.3500 - accuracy: 0.0344 - val_loss: 4.4317 - val_accuracy: 0.0375 - lr: 2.0000e-04\n",
      "Epoch 11/25\n",
      "100/100 [==============================] - 1505s 15s/step - loss: 4.3428 - accuracy: 0.0306 - val_loss: 4.3411 - val_accuracy: 0.0416 - lr: 4.0000e-05\n",
      "Epoch 12/25\n",
      "100/100 [==============================] - 1504s 15s/step - loss: 4.3013 - accuracy: 0.0350 - val_loss: 4.3155 - val_accuracy: 0.0397 - lr: 4.0000e-05\n",
      "Epoch 13/25\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.3017 - accuracy: 0.0288 \n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 1506s 15s/step - loss: 4.3017 - accuracy: 0.0288 - val_loss: 4.2585 - val_accuracy: 0.0409 - lr: 4.0000e-05\n",
      "Epoch 14/25\n",
      "100/100 [==============================] - 1496s 15s/step - loss: 4.2647 - accuracy: 0.0310 - val_loss: 4.2542 - val_accuracy: 0.0413 - lr: 8.0000e-06\n",
      "Epoch 15/25\n",
      "100/100 [==============================] - 1503s 15s/step - loss: 4.2794 - accuracy: 0.0325 - val_loss: 4.2357 - val_accuracy: 0.0437 - lr: 8.0000e-06\n",
      "Epoch 16/25\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.2801 - accuracy: 0.0375 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c3eacc5e4b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m            \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m            validation_steps=100 , callbacks=[lr_reduce])\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    870\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m    873\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/media/marco/DATA/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = int(20580*0.8)\n",
    "test_size = int(20580*0.2)\n",
    "\n",
    "hist = my_model.fit(\n",
    "           train_ds, steps_per_epoch=100, \n",
    "           epochs=25, validation_data=val_ds, \n",
    "           validation_steps=100 , callbacks=[lr_reduce])\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LOSS AND ACCURACY VISUALIZATION **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j5XxlVF8aWTT",
    "outputId": "9e91fd8b-3e8d-4a5d-b547-254f6bafd78f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5e8bb49715a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss']):\n",
    "    ax[i].plot(hist.history[met])\n",
    "    ax[i].plot(hist.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
